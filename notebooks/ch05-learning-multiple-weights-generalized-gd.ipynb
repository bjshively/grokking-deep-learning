{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f80b693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a2cc3",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 05 – Learning Multiple Weights (Generalized GD)\n",
    "\n",
    "> Work for this chapter of *Grokking Deep Learning*.\n",
    "\n",
    "## 1. Quick notes (5–10 bullets, in my own words)\n",
    "\n",
    "- Gradient descent is a flexible learning algorithm\n",
    "- It can be used for both multiple inputs and multiple outputs, rather than just 1 to 1\n",
    "- A neureal network can make multiple predictions with a single input\n",
    "- If inputs are of significantly different sizes/ranges (say, the number of bathrooms in a house and the square footage of a house) it can force you to use a slower learning rate to prevent divergence. This is a case where you may want to normalize the data so no single input dominates the calculation.\n",
    "- You can freeze the weight of a single input, and still minimize error through adjustments to other weights. This effectively shifts the curve in space, rather than moving the value along the curve.\n",
    "- `delta` is a measure of how much higher or lower you want a node's value to be (which direction to shift to minimize error)\n",
    "- `weight_delta` is a derivative-based estimate of direction and amount to adjust the delta for a given node, accounting for scaling, negative reversal, and stopping\n",
    "- The dot product of two vectors is a rough measure of similarity between two vectors (e.g. \"Is this a 2? A 1? A 9?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb89cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061325\n",
      "0.1017\n",
      "-0.373525\n",
      "0.0970425\n",
      "0.20013\n",
      "-0.005622500000000002\n",
      "-0.0054600000000000004\n",
      "1.30024\n",
      "0.08962\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Code from the book\n",
    "#\n",
    "# Recreate the main code examples from the chapter *by typing*,\n",
    "# not copy/paste. Keep them as close to the book as is reasonable.\n",
    "\n",
    "# Gradient descent with multiple inputs and outputs\n",
    "# 1 - DEFINE NETWORK\n",
    "# toes, % wins, # of fans\n",
    "weights = [[0.1, 0.1, -0.3], # hurt?\n",
    "           [0.1, 0.2, 0.0], # win?\n",
    "           [0.0, 1.3, 0.1] # sad?\n",
    "]\n",
    "# calculate weighted sum for input a, weight b\n",
    "def w_sum(a, b):\n",
    "    assert(len(a) == len(b))\n",
    "    output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += (a[i] * b[i])\n",
    "    return output\n",
    "\n",
    "def vect_mat_mul(vect,matrix):\n",
    "    assert(len(vect) == len(matrix))\n",
    "    output = [0,0,0]\n",
    "    for i in range(len(vect)):\n",
    "        output[i] = w_sum(vect,matrix[i])\n",
    "    return output\n",
    "\n",
    "def neural_network(inputs, weights):\n",
    "    pred = vect_mat_mul(inputs, weights)\n",
    "    return pred\n",
    "\n",
    "# 2 - MAKE A PREDICTION\n",
    "toes = [8.5, 9.5, 9.9, 9.0]\n",
    "wlrec = [0.65, 0.8, 0.8, 0.9]\n",
    "nfans = [1.2, 1.3, 0.5, 1.0]\n",
    "\n",
    "hurt = [0.1, 0.0, 0.0, 0.1]\n",
    "win = [1, 1, 0, 1]\n",
    "sad = [0.1, 0.0, 0.1, 0.2]\n",
    "\n",
    "alpha = 0.01\n",
    "inputs = [toes[0], wlrec[0], nfans[0]]\n",
    "true = [hurt[0], win[0], sad[0]]\n",
    "\n",
    "pred = neural_network(inputs, weights)\n",
    "error = [0, 0, 0]\n",
    "delta = [0, 0, 0]\n",
    "for i in range(len(true)):\n",
    "    error[i] = (pred[i] - true[i]) ** 2\n",
    "    delta[i] = pred[i] - true[i]\n",
    "\n",
    "# 3 - COMPARE RESULT\n",
    "def outer_prod(vec_a, vec_b):\n",
    "    out = np.zeros((len(vec_a), len(vec_b)))\n",
    "\n",
    "    for i in range(len(vec_a)):\n",
    "        for j in range(len(vec_b)):\n",
    "            out[i][j] = vec_a[i] * vec_b[j]\n",
    "    return out\n",
    "\n",
    "weight_deltas = outer_prod(inputs, delta)\n",
    "\n",
    "# 4 - UPDATE WEIGHTS\n",
    "for i in range(len(weights)):\n",
    "    for j in range(len(weights[0])):\n",
    "        weights[i][j] -= alpha * weight_deltas[i][j]\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    for j in range(len(weights[0])):\n",
    "        print(weights[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae78b4",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 2–3 sentence wrap-up\n",
    "\n",
    "- This chapter primarily demonstrated how different input and output shapes can utilize gradient descent to learn/calculate weights and minimize error.\n",
    "- The next chapter dives into practical applications, which should help clarify the methods.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
